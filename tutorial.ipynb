{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config IPython.matplotlib.backend = \"retina\"\n",
    "from matplotlib import rcParams\n",
    "rcParams[\"figure.dpi\"] = 150\n",
    "rcParams[\"savefig.dpi\"] = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import approx_fprime\n",
    "from scipy.linalg import solve_triangular, cho_solve\n",
    "\n",
    "import hemcee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a demo, we'll sample from a 10-dimensional covariant Gaussian.\n",
    "First, we need to define the log probability function and its gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum gradient error: 1.1157472671552426e-05\n"
     ]
    }
   ],
   "source": [
    "# Generate a random covariance matrix\n",
    "np.random.seed(42)\n",
    "ndim = 10\n",
    "L = np.random.randn(ndim, ndim)\n",
    "L[np.diag_indices_from(L)] = np.exp(L[np.diag_indices_from(L)])\n",
    "L[np.triu_indices_from(L, 1)] = 0.0\n",
    "cov = np.dot(L, L.T)\n",
    "\n",
    "def logprob(params):\n",
    "    alpha = solve_triangular(L, params, lower=True)\n",
    "    return -0.5*np.dot(alpha, alpha)\n",
    "\n",
    "def grad_logprob(params):\n",
    "    return -cho_solve((L, True), params)\n",
    "\n",
    "# Check the gradient numerically\n",
    "p0 = np.random.multivariate_normal(np.zeros(ndim), cov)\n",
    "print(\"Maximum gradient error: {0}\".format(\n",
    "    np.max(np.abs(approx_fprime(p0, logprob, 1e-8) - grad_logprob(p0)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we set up the sampler using these functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a dense metric that we will tune\n",
    "metric = hemcee.metric.DenseMetric(np.eye(ndim))\n",
    "\n",
    "# We will also tune the step size\n",
    "step = hemcee.step_size.VariableStepSize()\n",
    "\n",
    "# Set up the sampler\n",
    "sampler = hemcee.NoUTurnSampler(logprob, grad_logprob, step_size=step, metric=metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Hamiltonian samplers require a tuning phase (often called \"warmup\" or \"burn in\").\n",
    "During this phase, the step size and metric are automatically tuned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initial warm up: step_size: 2.8e-02; mean(accept_stat): 0.482: 100%|██████████| 100/100 [00:00<00:00, 292.35it/s]\n",
      "warm up 1/8: step_size: 1.0e-02; mean(accept_stat): 0.469: 100%|██████████| 25/25 [00:00<00:00, 304.89it/s]\n",
      "warm up 2/8: step_size: 9.1e-01; mean(accept_stat): 0.509: 100%|██████████| 25/25 [00:00<00:00, 305.51it/s]\n",
      "warm up 3/8: step_size: 1.6e+00; mean(accept_stat): 0.490: 100%|██████████| 50/50 [00:00<00:00, 460.63it/s]\n",
      "warm up 4/8: step_size: 2.4e+00; mean(accept_stat): 0.497: 100%|██████████| 100/100 [00:00<00:00, 660.03it/s]\n",
      "warm up 5/8: step_size: 6.5e-01; mean(accept_stat): 0.499: 100%|██████████| 200/200 [00:00<00:00, 607.44it/s]\n",
      "warm up 6/8: step_size: 1.3e+00; mean(accept_stat): 0.499: 100%|██████████| 400/400 [00:00<00:00, 893.68it/s]\n",
      "warm up 7/8: step_size: 1.3e+00; mean(accept_stat): 0.499: 100%|██████████| 800/800 [00:00<00:00, 870.41it/s]\n",
      "warm up 8/8: step_size: 1.3e+00; mean(accept_stat): 0.500: 100%|██████████| 3200/3200 [00:04<00:00, 765.98it/s]\n",
      "final warm up: step_size: 3.6e-01; mean(accept_stat): 0.497: 100%|██████████| 100/100 [00:00<00:00, 599.39it/s]\n"
     ]
    }
   ],
   "source": [
    "coords = np.random.randn(ndim)\n",
    "results = sampler.run_warmup(coords, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After burning in, we can run the production MCMC chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step_size: 8.9e-01; mean(accept_stat): 0.674: 100%|██████████| 5000/5000 [00:05<00:00, 901.41it/s]\n"
     ]
    }
   ],
   "source": [
    "coords_chain, logprob_chain = sampler.run_mcmc(results[0], 5000, initial_log_prob=results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the autocorrelation times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean autocorrelation time: 1.1840161367165014\n"
     ]
    }
   ],
   "source": [
    "taus = np.array([hemcee.autocorr.integrated_time(coords_chain[:, i])[0] for i in range(ndim)])\n",
    "print(\"Mean autocorrelation time: {0}\".format(np.mean(taus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
